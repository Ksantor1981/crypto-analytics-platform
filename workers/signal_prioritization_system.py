import json
import logging
import re
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import difflib
from enum import Enum

from improved_signal_parser import ImprovedSignal, ImprovedSignalExtractor, SignalDirection, SignalQuality

logger = logging.getLogger(__name__)

class PriorityLevel(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class DuplicateType(Enum):
    EXACT = "exact"
    SIMILAR = "similar"
    PARTIAL = "partial"
    NONE = "none"

@dataclass
class SignalPriority:
    signal: ImprovedSignal
    priority_score: float
    priority_level: PriorityLevel
    ranking_factors: Dict[str, float]
    duplicate_info: Dict[str, Any]
    group_id: Optional[str] = None
    recommendations: List[str] = None

@dataclass
class SignalGroup:
    group_id: str
    signals: List[ImprovedSignal]
    primary_signal: ImprovedSignal
    group_score: float
    group_type: str
    common_features: Dict[str, Any]
    recommendations: List[str]

@dataclass
class PrioritizationResult:
    prioritized_signals: List[SignalPriority]
    signal_groups: List[SignalGroup]
    duplicates_removed: int
    total_signals_processed: int
    priority_distribution: Dict[str, int]
    recommendations_summary: List[str]

class SignalPrioritizationSystem:
    def __init__(self):
        self.extractor = ImprovedSignalExtractor()
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
        self.priority_weights = {
            'confidence': 0.25,
            'quality': 0.20,
            'risk_reward': 0.15,
            'recency': 0.15,
            'source_reliability': 0.10,
            'asset_popularity': 0.05,
            'leverage': 0.05,
            'timeframe': 0.05
        }
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        self.priority_thresholds = {
            PriorityLevel.CRITICAL: 0.85,
            PriorityLevel.HIGH: 0.70,
            PriorityLevel.MEDIUM: 0.50,
            PriorityLevel.LOW: 0.30
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self.similarity_threshold = 0.8
        self.time_window_hours = 24
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞–∫—Ç–∏–≤—ã (–º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
        self.popular_assets = {
            'BTC': 1.0, 'ETH': 0.9, 'BNB': 0.8, 'ADA': 0.7, 'SOL': 0.7,
            'DOT': 0.6, 'AVAX': 0.6, 'MATIC': 0.6, 'LINK': 0.5, 'UNI': 0.5
        }
        
        # –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
        self.source_reliability = {
            'binance_killers': 0.8,
            'crypto_signals': 0.7,
            'trading_signals': 0.6,
            'reddit': 0.5,
            'tradingview': 0.6
        }

    def prioritize_signals(self, signals: List[ImprovedSignal]) -> PrioritizationResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é {len(signals)} —Å–∏–≥–Ω–∞–ª–æ–≤")
        
        # –®–∞–≥ 1: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_signals, duplicates_removed = self._filter_duplicates(signals)
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(unique_signals)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤")
        
        # –®–∞–≥ 2: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        prioritized_signals = []
        for signal in unique_signals:
            priority = self._calculate_signal_priority(signal)
            prioritized_signals.append(priority)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        prioritized_signals.sort(key=lambda x: x.priority_score, reverse=True)
        
        # –®–∞–≥ 3: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        signal_groups = self._group_similar_signals(prioritized_signals)
        
        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations_summary = self._generate_recommendations_summary(prioritized_signals, signal_groups)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        priority_distribution = Counter([p.priority_level.value for p in prioritized_signals])
        
        result = PrioritizationResult(
            prioritized_signals=prioritized_signals,
            signal_groups=signal_groups,
            duplicates_removed=duplicates_removed,
            total_signals_processed=len(signals),
            priority_distribution=dict(priority_distribution),
            recommendations_summary=recommendations_summary
        )
        
        logger.info(f"–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {priority_distribution.get('critical', 0)}, "
                   f"–í—ã—Å–æ–∫–∏—Ö: {priority_distribution.get('high', 0)}")
        
        return result

    def _filter_duplicates(self, signals: List[ImprovedSignal]) -> Tuple[List[ImprovedSignal], int]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤"""
        unique_signals = []
        duplicates_removed = 0
        seen_hashes = set()
        
        for signal in signals:
            # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞
            signal_hash = self._create_signal_hash(signal)
            
            if signal_hash not in seen_hashes:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Ö–æ–∂–∏–µ —Å–∏–≥–Ω–∞–ª—ã
                is_duplicate = False
                for existing_signal in unique_signals:
                    duplicate_type = self._check_similarity(signal, existing_signal)
                    if duplicate_type != DuplicateType.NONE:
                        is_duplicate = True
                        duplicates_removed += 1
                        logger.debug(f"–ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —Ç–∏–ø–∞ {duplicate_type.value}: {signal.asset} {signal.direction.value}")
                        break
                
                if not is_duplicate:
                    unique_signals.append(signal)
                    seen_hashes.add(signal_hash)
        
        return unique_signals, duplicates_removed

    def _create_signal_hash(self, signal: ImprovedSignal) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö–µ—à–∞ –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞"""
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        key_params = f"{signal.asset}_{signal.direction.value}_{signal.entry_price}_{signal.target_price}_{signal.stop_loss}"
        return hashlib.md5(key_params.encode()).hexdigest()

    def _check_similarity(self, signal1: ImprovedSignal, signal2: ImprovedSignal) -> DuplicateType:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
        time_diff = abs((signal1.timestamp - signal2.timestamp).total_seconds() / 3600)
        if time_diff > self.time_window_hours:
            return DuplicateType.NONE
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if (signal1.asset == signal2.asset and 
            signal1.direction == signal2.direction and
            abs(signal1.entry_price - signal2.entry_price) < 0.01):
            return DuplicateType.EXACT
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        text_similarity = self._calculate_text_similarity(
            signal1.cleaned_text, signal2.cleaned_text
        )
        
        if text_similarity > self.similarity_threshold:
            return DuplicateType.SIMILAR
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if (signal1.asset == signal2.asset and 
            signal1.direction == signal2.direction and
            abs(signal1.entry_price - signal2.entry_price) / signal1.entry_price < 0.05):
            return DuplicateType.PARTIAL
        
        return DuplicateType.NONE

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not text1 or not text2:
            return 0.0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity = difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        return similarity

    def _calculate_signal_priority(self, signal: ImprovedSignal) -> SignalPriority:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Å–∏–≥–Ω–∞–ª–∞"""
        factors = {}
        
        # –§–∞–∫—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        factors['confidence'] = (signal.real_confidence + signal.calculated_confidence) / 2
        
        # –§–∞–∫—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
        quality_scores = {
            SignalQuality.EXCELLENT: 1.0,
            SignalQuality.GOOD: 0.8,
            SignalQuality.BASIC: 0.6,
            SignalQuality.POOR: 0.4,
            SignalQuality.BASIC: 0.5
        }
        factors['quality'] = quality_scores.get(signal.signal_quality, 0.5)
        
        # –§–∞–∫—Ç–æ—Ä —Ä–∏—Å–∫/–ø—Ä–∏–±—ã–ª—å
        if signal.risk_reward_ratio and signal.risk_reward_ratio > 0:
            factors['risk_reward'] = min(signal.risk_reward_ratio / 3.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 1.0
        else:
            factors['risk_reward'] = 0.5
        
        # –§–∞–∫—Ç–æ—Ä –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
        if signal.timestamp:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ timestamp —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ datetime –æ–±—ä–µ–∫—Ç–æ–º
                if isinstance(signal.timestamp, str):
                    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –≤ datetime
                    if 'T' in signal.timestamp:
                        # ISO —Ñ–æ—Ä–º–∞—Ç
                        timestamp_dt = datetime.fromisoformat(signal.timestamp.replace('Z', '+00:00'))
                    else:
                        # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
                        timestamp_dt = datetime.strptime(signal.timestamp, '%Y-%m-%d %H:%M:%S')
                else:
                    # –£–∂–µ datetime –æ–±—ä–µ–∫—Ç
                    timestamp_dt = signal.timestamp
                
                hours_old = (datetime.now() - timestamp_dt).total_seconds() / 3600
                factors['recency'] = max(0, 1 - hours_old / 24)  # –°–Ω–∏–∂–∞–µ—Ç—Å—è –∑–∞ 24 —á–∞—Å–∞
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ timestamp {signal.timestamp}: {e}")
                factors['recency'] = 0.5
        else:
            factors['recency'] = 0.5
        
        # –§–∞–∫—Ç–æ—Ä –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source = signal.channel.lower()
        factors['source_reliability'] = self.source_reliability.get(source, 0.5)
        
        # –§–∞–∫—Ç–æ—Ä –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∞–∫—Ç–∏–≤–∞
        factors['asset_popularity'] = self.popular_assets.get(signal.asset, 0.3)
        
        # –§–∞–∫—Ç–æ—Ä –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –ø–ª–µ—á–∞
        if signal.leverage:
            factors['leverage'] = min(signal.leverage / 10.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 1.0
        else:
            factors['leverage'] = 0.5
        
        # –§–∞–∫—Ç–æ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞
        timeframe_scores = {
            '1m': 0.3, '5m': 0.4, '15m': 0.5, '30m': 0.6,
            '1h': 0.7, '4h': 0.8, '1d': 0.9, '1w': 1.0
        }
        factors['timeframe'] = timeframe_scores.get(signal.timeframe, 0.5)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        priority_score = sum(factors[factor] * self.priority_weights[factor] 
                           for factor in self.priority_weights)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        priority_level = PriorityLevel.LOW
        for level, threshold in self.priority_thresholds.items():
            if priority_score >= threshold:
                priority_level = level
                break
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        duplicate_info = {
            'type': DuplicateType.NONE.value,
            'similar_signals': 0
        }
        
        return SignalPriority(
            signal=signal,
            priority_score=priority_score,
            priority_level=priority_level,
            ranking_factors=factors,
            duplicate_info=duplicate_info,
            recommendations=[]
        )

    def _group_similar_signals(self, prioritized_signals: List[SignalPriority]) -> List[SignalGroup]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
        groups = []
        processed_signals = set()
        
        for i, priority_signal in enumerate(prioritized_signals):
            if i in processed_signals:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
            group_signals = [priority_signal.signal]
            processed_signals.add(i)
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–∏–≥–Ω–∞–ª—ã
            for j, other_priority_signal in enumerate(prioritized_signals[i+1:], i+1):
                if j in processed_signals:
                    continue
                
                if self._signals_belong_to_group(priority_signal.signal, other_priority_signal.signal):
                    group_signals.append(other_priority_signal.signal)
                    processed_signals.add(j)
            
            if len(group_signals) > 1:
                # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—É
                group = self._create_signal_group(group_signals)
                groups.append(group)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º group_id –¥–ª—è –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –≥—Ä—É–ø–ø–µ
                for signal in group_signals:
                    for priority_signal in prioritized_signals:
                        if priority_signal.signal.id == signal.id:
                            priority_signal.group_id = group.group_id
                            break
        
        return groups

    def _signals_belong_to_group(self, signal1: ImprovedSignal, signal2: ImprovedSignal) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ª–∏ —Å–∏–≥–Ω–∞–ª—ã –∫ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø–µ"""
        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–π –∞–∫—Ç–∏–≤ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        if signal1.asset != signal2.asset or signal1.direction != signal2.direction:
            return False
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 6 —á–∞—Å–æ–≤)
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º timestamp –¥–ª—è –æ–±–æ–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
            def parse_timestamp(ts):
                if isinstance(ts, str):
                    if 'T' in ts:
                        return datetime.fromisoformat(ts.replace('Z', '+00:00'))
                    else:
                        return datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')
                return ts
            
            ts1 = parse_timestamp(signal1.timestamp)
            ts2 = parse_timestamp(signal2.timestamp)
            
            time_diff = abs((ts1 - ts2).total_seconds() / 3600)
            if time_diff > 6:
                return False
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è timestamp: {e}")
            return False
        
        # –°—Ö–æ–∂–∏–µ —Ü–µ–Ω—ã –≤—Ö–æ–¥–∞ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 5%)
        if abs(signal1.entry_price - signal2.entry_price) / signal1.entry_price > 0.05:
            return False
        
        return True

    def _create_signal_group(self, signals: List[ImprovedSignal]) -> SignalGroup:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤"""
        group_id = f"group_{signals[0].asset}_{signals[0].direction.value}_{int(signals[0].timestamp.timestamp())}"
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∏–≥–Ω–∞–ª (—Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º)
        primary_signal = max(signals, key=lambda s: s.real_confidence)
        
        # –û–±—â–∏–π —Å—á–µ—Ç –≥—Ä—É–ø–ø—ã
        group_score = sum(s.real_confidence for s in signals) / len(signals)
        
        # –¢–∏–ø –≥—Ä—É–ø–ø—ã
        if len(signals) >= 3:
            group_type = "strong_consensus"
        elif len(signals) == 2:
            group_type = "moderate_consensus"
        else:
            group_type = "single_signal"
        
        # –û–±—â–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        common_features = {
            'asset': signals[0].asset,
            'direction': signals[0].direction.value,
            'avg_entry_price': sum(s.entry_price for s in signals) / len(signals),
            'avg_confidence': sum(s.real_confidence for s in signals) / len(signals),
            'signal_count': len(signals)
        }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã
        recommendations = self._generate_group_recommendations(signals, group_type)
        
        return SignalGroup(
            group_id=group_id,
            signals=signals,
            primary_signal=primary_signal,
            group_score=group_score,
            group_type=group_type,
            common_features=common_features,
            recommendations=recommendations
        )

    def _generate_recommendations_summary(self, prioritized_signals: List[SignalPriority], 
                                        signal_groups: List[SignalGroup]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        critical_count = sum(1 for s in prioritized_signals if s.priority_level == PriorityLevel.CRITICAL)
        high_count = sum(1 for s in prioritized_signals if s.priority_level == PriorityLevel.HIGH)
        
        if critical_count > 0:
            recommendations.append(f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {critical_count}. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ.")
        
        if high_count > 3:
            recommendations.append(f"üìà –ú–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ ({high_count}). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.")
        
        # –ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø
        strong_groups = [g for g in signal_groups if g.group_type == "strong_consensus"]
        if strong_groups:
            recommendations.append(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(strong_groups)} –≥—Ä—É–ø–ø —Å —Å–∏–ª—å–Ω—ã–º –∫–æ–Ω—Å–µ–Ω—Å—É—Å–æ–º. –í—ã—Å–æ–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å.")
        
        # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–æ–≤
        asset_counts = Counter(s.signal.asset for s in prioritized_signals)
        top_assets = asset_counts.most_common(3)
        if top_assets:
            recommendations.append(f"üî• –¢–æ–ø –∞–∫—Ç–∏–≤—ã: {', '.join(f'{asset} ({count})' for asset, count in top_assets)}")
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_counts = Counter(s.signal.direction.value for s in prioritized_signals)
        if direction_counts:
            dominant_direction = max(direction_counts.items(), key=lambda x: x[1])
            recommendations.append(f"üìä –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {dominant_direction[0]} ({dominant_direction[1]} —Å–∏–≥–Ω–∞–ª–æ–≤)")
        
        return recommendations

    def _generate_group_recommendations(self, signals: List[ImprovedSignal], group_type: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –≥—Ä—É–ø–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤"""
        recommendations = []
        
        if group_type == "strong_consensus":
            recommendations.append("‚úÖ –°–∏–ª—å–Ω—ã–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å - –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞")
            recommendations.append("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏")
        elif group_type == "moderate_consensus":
            recommendations.append("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏")
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω
        avg_entry = sum(s.entry_price for s in signals) / len(signals)
        price_variance = sum((s.entry_price - avg_entry) ** 2 for s in signals) / len(signals)
        
        if price_variance < 0.01:  # –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è —Ü–µ–Ω
            recommendations.append("üéØ –¶–µ–Ω—ã –≤—Ö–æ–¥–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã - —á–µ—Ç–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤—Ö–æ–¥–∞")
        else:
            recommendations.append("üìä –†–∞–∑–±—Ä–æ—Å —Ü–µ–Ω –≤—Ö–æ–¥–∞ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ä–µ–¥–Ω—é—é —Ü–µ–Ω—É")
        
        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        avg_confidence = sum(s.real_confidence for s in signals) / len(signals)
        if avg_confidence > 0.8:
            recommendations.append("üöÄ –í—ã—Å–æ–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã")
        elif avg_confidence < 0.5:
            recommendations.append("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø—ã - –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å")
        
        return recommendations

    def save_prioritization_results(self, result: PrioritizationResult, filename: str = "prioritized_signals.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            serializable_result = {
                'prioritized_signals': [
                    {
                        'signal': asdict(p.signal),
                        'priority_score': p.priority_score,
                        'priority_level': p.priority_level.value,
                        'ranking_factors': p.ranking_factors,
                        'duplicate_info': p.duplicate_info,
                        'group_id': p.group_id,
                        'recommendations': p.recommendations
                    }
                    for p in result.prioritized_signals
                ],
                'signal_groups': [
                    {
                        'group_id': g.group_id,
                        'signals': [asdict(s) for s in g.signals],
                        'primary_signal': asdict(g.primary_signal),
                        'group_score': g.group_score,
                        'group_type': g.group_type,
                        'common_features': g.common_features,
                        'recommendations': g.recommendations
                    }
                    for g in result.signal_groups
                ],
                'statistics': {
                    'duplicates_removed': result.duplicates_removed,
                    'total_signals_processed': result.total_signals_processed,
                    'priority_distribution': result.priority_distribution,
                    'recommendations_summary': result.recommendations_summary
                },
                'timestamp': datetime.now().isoformat()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(serializable_result, f, indent=2, ensure_ascii=False)
            
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏: {e}")

def main():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logging.basicConfig(level=logging.INFO)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    test_signals = [
        ImprovedSignal(
            id="1", asset="BTC", direction=SignalDirection.LONG,
            entry_price=45000, target_price=48000, stop_loss=44000,
            leverage=2, timeframe="4h", channel="binance_killers",
            message_id="123", original_text="BTC LONG 45000", cleaned_text="BTC LONG 45000",
            timestamp=datetime.now(), extraction_time=datetime.now(),
            signal_quality=SignalQuality.GOOD, real_confidence=0.8, calculated_confidence=0.75,
            bybit_available=True, is_valid=True, validation_errors=[],
            risk_reward_ratio=2.5, potential_profit=3000, potential_loss=1000
        ),
        ImprovedSignal(
            id="2", asset="BTC", direction=SignalDirection.LONG,
            entry_price=45100, target_price=48500, stop_loss=44100,
            leverage=3, timeframe="4h", channel="crypto_signals",
            message_id="124", original_text="BTC LONG 45100", cleaned_text="BTC LONG 45100",
            timestamp=datetime.now(), extraction_time=datetime.now(),
            signal_quality=SignalQuality.EXCELLENT, real_confidence=0.9, calculated_confidence=0.85,
            bybit_available=True, is_valid=True, validation_errors=[],
            risk_reward_ratio=3.0, potential_profit=3400, potential_loss=1000
        ),
        ImprovedSignal(
            id="3", asset="ETH", direction=SignalDirection.SHORT,
            entry_price=3200, target_price=3000, stop_loss=3300,
            leverage=2, timeframe="1h", channel="trading_signals",
            message_id="125", original_text="ETH SHORT 3200", cleaned_text="ETH SHORT 3200",
            timestamp=datetime.now(), extraction_time=datetime.now(),
            signal_quality=SignalQuality.AVERAGE, real_confidence=0.6, calculated_confidence=0.55,
            bybit_available=True, is_valid=True, validation_errors=[],
            risk_reward_ratio=2.0, potential_profit=200, potential_loss=100
        )
    ]
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
    prioritization_system = SignalPrioritizationSystem()
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã
    result = prioritization_system.prioritize_signals(test_signals)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    prioritization_system.save_prioritization_results(result)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ ===")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {result.total_signals_processed}")
    print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {result.duplicates_removed}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {result.priority_distribution}")
    print(f"–°–æ–∑–¥–∞–Ω–æ –≥—Ä—É–ø–ø: {len(result.signal_groups)}")
    
    print(f"\n=== –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===")
    for rec in result.recommendations_summary:
        print(f"‚Ä¢ {rec}")
    
    print(f"\n=== –¢–æ–ø —Å–∏–≥–Ω–∞–ª—ã ===")
    for i, priority_signal in enumerate(result.prioritized_signals[:3]):
        signal = priority_signal.signal
        print(f"{i+1}. {signal.asset} {signal.direction.value} @ {signal.entry_price}")
        print(f"   –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority_signal.priority_level.value} ({priority_signal.priority_score:.2f})")
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {signal.channel}")
        print()

if __name__ == "__main__":
    main()
